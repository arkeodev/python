{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapegraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScrapeGraphAI is a cutting-edge Python library designed for web scraping. It leverages large language models (LLM) and direct graph logic to create efficient and effective scraping pipelines for websites, documents, and XML files. Simply specify the information you want to extract, and ScrapeGraphAI will handle the rest, making web scraping accessible and straightforward for users of all levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with ScrapeGraphAI, you can easily install it via pip. The reference page for ScrapeGraphAI is available on [PyPI](https://pypi.org/project/scrapegraphai/).\n",
    "\n",
    "```sh\n",
    "pip install scrapegraphai\n",
    "```\n",
    "\n",
    "Additionally, for JavaScript-based scraping, you will need to install Playwright:\n",
    "\n",
    "```sh\n",
    "playwright install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main scraping components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nodes are the fundamental building blocks of ScrapeGraphAI's scraping pipelines. Each node represents a specific task within the pipeline, such as fetching data from a URL, extracting content using a pattern, or processing the extracted data. Nodes can be configured to perform a variety of actions, making them highly versatile and adaptable to different scraping scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the types of nodes and their functionalitien:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BaseNode Module**: Provides an abstract base class for nodes in a graph-based workflow, designed to perform specific actions when executed.\n",
    "\n",
    "- **ConditionalNode Module**: Determines the next step in the graph's execution flow based on the presence and content of a specified key in the graph's state.\n",
    "\n",
    "- **FetchNode Module**: Responsible for fetching the HTML content of a specified URL or loading various types of documents and updating the graph's state with this content.\n",
    "\n",
    "- **GenerateAnswerCSVNode Module**: Generates an answer using a language model (LLM) based on the user's input and the content extracted from a webpage, and constructs a prompt for the LLM.\n",
    "\n",
    "- **GenerateAnswerNode Module**: Similar to GenerateAnswerCSVNode, it generates an answer using a large language model (LLM) based on the user's input and the content extracted from a webpage.\n",
    "\n",
    "- **GenerateAnswerOmniNode Module**: Generates an answer using a large language model (LLM) based on the user's input and the content extracted from a webpage, similar to GenerateAnswerNode.\n",
    "\n",
    "- **GenerateAnswerPDFNode Module**: Generates an answer using a language model (LLM) based on the user's input and the content extracted from a webpage, similar to GenerateAnswerNode.\n",
    "\n",
    "- **GenerateScraperNode Module**: Generates a Python script for scraping a website using the specified library, based on the user's prompt and the scraped content.\n",
    "\n",
    "- **GetProbableTagsNode Module**: Utilizes a language model to identify probable HTML tags within a document that are likely to contain information relevant to a user's query.\n",
    "\n",
    "- **ImageToTextNode Module**: Retrieves images from a list of URLs and returns a description of the images using an image-to-text model.\n",
    "\n",
    "- **MergeAnswersNode Module**: Merges the answers from multiple graph instances into a single answer.\n",
    "\n",
    "- **ParseNode Module**: Parses HTML content from a document and splits it into chunks for further processing.\n",
    "\n",
    "- **RAGNode Module**: Compresses input tokens and stores the document in a vector database for retrieval, storing relevant chunks in the state.\n",
    "\n",
    "- **RobotsNode Module**: Checks if a website is scrapeable based on the robots.txt file, using a language model to determine if scraping is allowed.\n",
    "\n",
    "- **SearchInternetNode Module**: Generates a search query based on the user's input, searches the internet for relevant information, and updates the state with the generated answer.\n",
    "\n",
    "- **SearchLinkNode Module**: Filters out relevant links in the webpage content based on the user prompt, ideal to use after the FetchNode.\n",
    "\n",
    "- **TextToSpeechNode Module**: Converts text to speech using the specified text-to-speech model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs in ScrapeGraphAI represent the overall structure and flow of a scraping pipeline. They consist of interconnected nodes, each performing a distinct task and passing data to the next node in the sequence. The graph-based approach allows for flexible and modular design, enabling users to easily modify and extend their scraping pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components of a Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Nodes**: As described earlier, nodes are the individual tasks within the graph. Each node is defined by its type and specific configuration settings.\n",
    "   \n",
    "2. **Edges**: Edges define the connections between nodes, determining the order of execution and the flow of data through the pipeline. Edges ensure that data is passed correctly from one node to the next, maintaining the integrity of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the types of graphs and their functionalities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **AbstractGraph Module**: Provides a scaffolding class for creating a graph representation and executing it.\n",
    "\n",
    "- **BaseGraph Module**: Provides a class for managing and executing a graph composed of interconnected nodes.\n",
    "\n",
    "- **CSVScraperGraph Module**: Defines a class for creating and executing a graph that automates the process of extracting information from web pages using a natural language model.\n",
    "\n",
    "- **JSONScraperGraph Module**: Defines a class for creating and executing a graph that automates the process of extracting information from JSON files using a natural language model.\n",
    "\n",
    "- **OmniScraperGraph Module**: Defines a class for creating and executing a graph that automates the process of extracting information from web pages using a natural language model.\n",
    "\n",
    "- **OmniSearchGraph Module**: Defines a class for creating and executing a graph that searches the internet for answers to a given prompt, combining web scraping and internet searching.\n",
    "\n",
    "- **PDFScraperGraph Module**: Defines a class for creating and executing a graph that extracts information from PDF files using a natural language model to interpret and answer prompts.\n",
    "\n",
    "- **ScriptCreatorGraph Module**: Defines a class for creating and executing a graph that generates web scraping scripts.\n",
    "\n",
    "- **SearchGraph Module**: Defines a class for creating and executing a graph that searches the internet for answers to a given prompt.\n",
    "\n",
    "- **SmartScraperGraph Module**: Defines a class for creating and executing a graph that automates the process of extracting information from web pages using a natural language model to interpret and answer prompts.\n",
    "\n",
    "- **SpeechGraph Module**: Defines a class for creating and executing a graph that scrapes the web, provides an answer to a given prompt, and generates an audio file.\n",
    "\n",
    "- **XMLScraperGraph Module**: Defines a class for creating and executing a graph that extracts information from XML files using a natural language model to interpret and answer prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing a Scraping Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing a scraping graph, consider the following best practices:\n",
    "\n",
    "1. **Define Clear Objectives**: Start by clearly defining the objectives of your scraping task. Identify the data you need to extract and the sources from which it will be fetched.\n",
    "   \n",
    "2. **Modular Approach**: Break down the scraping task into smaller, manageable nodes. This modular approach allows for easy debugging, maintenance, and scalability.\n",
    "\n",
    "3. **Optimize Data Flow**: Arrange nodes in a logical sequence to optimize the flow of data. Ensure that each node performs its task efficiently and passes the data correctly to the next node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Graph Configuration:**\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text\": [\n",
    "        {\n",
    "            \"nodes\": [\n",
    "                {\n",
    "                    \"node_name\": \"SearchInternetNode\",\n",
    "                    \"node_type\": \"node\"\n",
    "                },\n",
    "                {\n",
    "                    \"node_name\": \"FetchNode\",\n",
    "                    \"node_type\": \"node\"\n",
    "                },\n",
    "                {\n",
    "                    \"node_name\": \"RAGNode\",\n",
    "                    \"node_type\": \"node\"\n",
    "                },\n",
    "                {\n",
    "                    \"node_name\": \"ParseNode\",\n",
    "                    \"node_type\": \"node\"\n",
    "                }\n",
    "            ],\n",
    "            \"edges\": [\n",
    "                {\n",
    "                    \"from\": \"SearchInternetNode\",\n",
    "                    \"to\": [\n",
    "                        \"FetchNode\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"FetchNode\",\n",
    "                    \"to\": [\n",
    "                        \"RAGNode\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"RAGNode\",\n",
    "                    \"to\": [\n",
    "                        \"ParseNode\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"entry_point\": \"SearchInternetNode\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the graph consists of four nodes connected by edges in a sequence:\n",
    "\n",
    "**Nodes**\n",
    "\n",
    "1. **SearchInternetNode**: This node is responsible for generating a search query based on the user's input and searching the internet for relevant information. It acts as the starting point of the graph, initiating the process by generating the necessary search queries.\n",
    "\n",
    "2. **FetchNode**: After the search query is generated, the FetchNode retrieves the HTML content of the specified URLs found from the internet search. This node fetches the actual web content that will be processed further.\n",
    "\n",
    "3. **RAGNode**: Following the fetch operation, the RAGNode compresses the input tokens and stores the document in a vector database for retrieval. It ensures that relevant chunks of data are stored in the state for efficient processing and retrieval.\n",
    "\n",
    "4. **ParseNode**: Finally, the ParseNode parses the HTML content from the document and splits it into chunks for further processing. This node extracts and organizes the relevant data into manageable pieces for the next steps in the pipeline.\n",
    "\n",
    "**Edges**\n",
    "\n",
    "The edges define the flow of data between the nodes:\n",
    "\n",
    "- The graph starts at the **SearchInternetNode**.\n",
    "- The output of the **SearchInternetNode** is passed to the **FetchNode**.\n",
    "- The **FetchNode** then sends its fetched content to the **RAGNode**.\n",
    "- Finally, the **RAGNode** processes the data and passes it to the **ParseNode**.\n",
    "\n",
    "**Entry Point**\n",
    "\n",
    "The entry point of this graph is the **SearchInternetNode**, indicating that the graph's execution begins with generating the search query and proceeds through the sequence of nodes to parse the fetched and processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples from Scrapegraph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples below are totally taken from the ScrapeGraphAI github repo: \n",
    "\n",
    "https://github.com/VinciGit00/Scrapegraph-ai/tree/ff4ccb94a125193efcf6a3c71781faf50d0464c3/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install scrapegraphai --upgrade\n",
    "! apt install chromium-chromedriver\n",
    "! pip install nest_asyncio\n",
    "! pip install playwright\n",
    "! playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# enter Open AI key\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is getting an error in RobotsNode. A support message is sent to the contributers of the ScrapegraphAI from the Discord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of custom graph using existing nodes\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from scrapegraphai.models import OpenAI\n",
    "from scrapegraphai.graphs import BaseGraph\n",
    "from scrapegraphai.nodes import FetchNode, ParseNode, RAGNode, GenerateAnswerNode, RobotsNode\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"openai_api_key\": openai_key,\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Define the graph nodes\n",
    "# ************************************************\n",
    "\n",
    "llm_model = OpenAI(graph_config[\"llm\"])\n",
    "embedder = OpenAIEmbeddings(api_key=llm_model.openai_api_key)\n",
    "\n",
    "# define the nodes for the graph\n",
    "robot_node = RobotsNode(\n",
    "    input=\"url\",\n",
    "    output=[\"is_scrapable\"],\n",
    "    node_config={\n",
    "        \"llm_model\": llm_model,\n",
    "        \"force_scraping\": True,\n",
    "        \"verbose\": True,\n",
    "        }\n",
    ")\n",
    "\n",
    "fetch_node = FetchNode(\n",
    "    input=\"url | local_dir\",\n",
    "    output=[\"doc\", \"link_urls\", \"img_urls\"],\n",
    "    node_config={\n",
    "        \"verbose\": True,\n",
    "        \"headless\": True,\n",
    "    }\n",
    ")\n",
    "parse_node = ParseNode(\n",
    "    input=\"doc\",\n",
    "    output=[\"parsed_doc\"],\n",
    "    node_config={\n",
    "        \"chunk_size\": 4096,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    ")\n",
    "rag_node = RAGNode(\n",
    "    input=\"user_prompt & (parsed_doc | doc)\",\n",
    "    output=[\"relevant_chunks\"],\n",
    "    node_config={\n",
    "        \"llm_model\": llm_model,\n",
    "        \"embedder_model\": embedder,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    ")\n",
    "generate_answer_node = GenerateAnswerNode(\n",
    "    input=\"user_prompt & (relevant_chunks | parsed_doc | doc)\",\n",
    "    output=[\"answer\"],\n",
    "    node_config={\n",
    "        \"llm_model\": llm_model,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ************************************************\n",
    "# Create the graph by defining the connections\n",
    "# ************************************************\n",
    "\n",
    "graph = BaseGraph(\n",
    "    nodes=[\n",
    "        robot_node,\n",
    "        fetch_node,\n",
    "        parse_node,\n",
    "        rag_node,\n",
    "        generate_answer_node,\n",
    "    ],\n",
    "    edges=[\n",
    "        (robot_node, fetch_node),\n",
    "        (fetch_node, parse_node),\n",
    "        (parse_node, rag_node),\n",
    "        (rag_node, generate_answer_node)\n",
    "    ],\n",
    "    entry_point=robot_node\n",
    ")\n",
    "\n",
    "# ************************************************\n",
    "# Execute the graph\n",
    "# ************************************************\n",
    "\n",
    "result, execution_info = graph.execute({\n",
    "    \"user_prompt\": \"Describe the content\",\n",
    "    \"url\": \"https://example.com/\"\n",
    "})\n",
    "\n",
    "# get the answer from the result\n",
    "result = result.get(\"answer\", \"No answer found.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigate the links of a URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepScraper is a scraping pipeline that automates the process of \n",
    "extracting information from web pages using a natural language model \n",
    "to interpret and answer prompts.\n",
    "\n",
    "Unlike SmartScraper, DeepScraper can navigate to the links within,\n",
    "the input webpage to fuflfil the task within the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This graph is still in the WIP and can produce some errors during the run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scrapegraphai.graphs import DeepScraperGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-4\",\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "    \"max_depth\": 1\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the SmartScraperGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "deep_scraper_graph = DeepScraperGraph(\n",
    "    prompt=\"List me all the job titles and detailed job description.\",\n",
    "    # also accepts a string with the already downloaded HTML code\n",
    "    source=\"https://www.google.com/about/careers/applications/jobs/results/?location=Bangalore%20India\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = deep_scraper_graph.run()\n",
    "print(result)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution inf\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = deep_scraper_graph.get_execution_info()\n",
    "print(f\"Relevant_links: {deep_scraper_graph.get_state('relevant_links')}\")\n",
    "print(prettify_exec_info(graph_exec_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CSVScraperGraph` is distinct from other graph types in that it is specifically designed to scrape and process data from CSV files or directories containing CSV files. This smart scraper uses a natural language model to interpret prompts and extract relevant information from the CSV data. The `source` parameter can be a single CSV file path or a directory containing multiple CSV files, thus allowing flexible and comprehensive data extraction from CSV formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scrapegraphai.graphs import CSVScraperGraph\n",
    "from scrapegraphai.utils import convert_to_csv, convert_to_json, prettify_exec_info\n",
    "\n",
    "# ************************************************\n",
    "# Read the CSV file\n",
    "# ************************************************\n",
    "\n",
    "FILE_NAME = \"inputs/username.csv\"\n",
    "curr_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "file_path = os.path.join(curr_dir, FILE_NAME)\n",
    "\n",
    "text = pd.read_csv(file_path)\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the CSVScraperGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "csv_scraper_graph = CSVScraperGraph(\n",
    "    prompt=\"List me all the last names\",\n",
    "    source=str(text),  # Pass the content of the file, not the file object\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = csv_scraper_graph.run()\n",
    "print(result)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = csv_scraper_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n",
    "\n",
    "# Save to json or csv\n",
    "convert_to_csv(result, \"result\")\n",
    "convert_to_json(result, \"result\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `JSONScraperGraph` is specialized for extracting data from JSON files, differentiating it from other graph types by focusing on structured JSON data rather than web pages or CSV files. The `source` parameter can be either a path to a single JSON file or a directory containing multiple JSON files. This flexibility allows the graph to process and scrape data from various JSON sources effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic example of scraping pipeline using JSONScraperGraph from JSON documents\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from scrapegraphai.graphs import JSONScraperGraph\n",
    "from scrapegraphai.utils import convert_to_csv, convert_to_json, prettify_exec_info\n",
    "\n",
    "# ************************************************\n",
    "# Read the JSON file\n",
    "# ************************************************\n",
    "\n",
    "FILE_NAME = \"inputs/example.json\"\n",
    "curr_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "file_path = os.path.join(curr_dir, FILE_NAME)\n",
    "\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the JSONScraperGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "json_scraper_graph = JSONScraperGraph(\n",
    "    prompt=\"List me all the channel titles, title and descriptions of the youtube videos\",\n",
    "    source=text,  # Pass the content of the file, not the file object\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = json_scraper_graph.run()\n",
    "# Print the JSON data in a pretty format\n",
    "pretty_json = json.dumps(result, indent=4)\n",
    "print(pretty_json)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = json_scraper_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n",
    "\n",
    "# Save to json or csv\n",
    "convert_to_csv(result, \"result\")\n",
    "convert_to_json(result, \"result\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph scrapes the XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"books\": [\n",
      "        {\n",
      "            \"author\": \"Gambardella, Matthew\",\n",
      "            \"title\": \"XML Developer's Guide\",\n",
      "            \"genre\": \"Computer\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Ralls, Kim\",\n",
      "            \"title\": \"Midnight Rain\",\n",
      "            \"genre\": \"Fantasy\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Corets, Eva\",\n",
      "            \"title\": \"Maeve Ascendant\",\n",
      "            \"genre\": \"Fantasy\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Corets, Eva\",\n",
      "            \"title\": \"Oberon's Legacy\",\n",
      "            \"genre\": \"Fantasy\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Corets, Eva\",\n",
      "            \"title\": \"The Sundered Grail\",\n",
      "            \"genre\": \"Fantasy\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Randall, Cynthia\",\n",
      "            \"title\": \"Lover Birds\",\n",
      "            \"genre\": \"Romance\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Thurman, Paula\",\n",
      "            \"title\": \"Splish Splash\",\n",
      "            \"genre\": \"Romance\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Knorr, Stefan\",\n",
      "            \"title\": \"Creepy Crawlies\",\n",
      "            \"genre\": \"Horror\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Kress, Peter\",\n",
      "            \"title\": \"Paradox Lost\",\n",
      "            \"genre\": \"Science Fiction\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"O'Brien, Tim\",\n",
      "            \"title\": \"Microsoft .NET: The Programming Bible\",\n",
      "            \"genre\": \"Computer\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"O'Brien, Tim\",\n",
      "            \"title\": \"MSXML3: A Comprehensive Guide\",\n",
      "            \"genre\": \"Computer\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"Galos, Mike\",\n",
      "            \"title\": \"Visual Studio 7: A Comprehensive Guide\",\n",
      "            \"genre\": \"Computer\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "        node_name  total_tokens  prompt_tokens  completion_tokens  \\\n",
      "0           Fetch             0              0                  0   \n",
      "1             RAG             0              0                  0   \n",
      "2  GenerateAnswer          1720           1318                402   \n",
      "3    TOTAL RESULT          1720           1318                402   \n",
      "\n",
      "   successful_requests  total_cost_USD  exec_time  \n",
      "0                    0        0.000000   0.000355  \n",
      "1                    0        0.000000   1.298339  \n",
      "2                    1        0.002781   6.141925  \n",
      "3                    1        0.002781   7.440619  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic example of scraping pipeline using XMLScraperGraph from XML documents\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from scrapegraphai.graphs import XMLScraperGraph\n",
    "from scrapegraphai.utils import convert_to_csv, convert_to_json, prettify_exec_info\n",
    "\n",
    "# ************************************************\n",
    "# Read the XML file\n",
    "# ************************************************\n",
    "\n",
    "FILE_NAME = \"inputs/books.xml\"\n",
    "curr_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "file_path = os.path.join(curr_dir, FILE_NAME)\n",
    "\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "    \"verbose\":False,\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the XMLScraperGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "xml_scraper_graph = XMLScraperGraph(\n",
    "    prompt=\"List me all the authors, title and genres of the books\",\n",
    "    source=text,  # Pass the content of the file, not the file object\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = xml_scraper_graph.run()\n",
    "# Print the JSON data in a pretty format\n",
    "pretty_json = json.dumps(result, indent=4)\n",
    "print(pretty_json)\n",
    "\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = xml_scraper_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n",
    "\n",
    "# Save to json or csv\n",
    "convert_to_csv(result, \"result\")\n",
    "convert_to_json(result, \"result\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping URL's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OmniScraperGraph` is distinct from other graph types by its ability to extract and integrate diverse data types, including text and images, from web pages or local directories, using a natural language model to process and respond to prompts comprehensively. The`source` parameter can be either a URL starting with \"http\" for web pages or a local directory path, allowing it to handle various input sources seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Basic example of scraping pipeline using OmniScraper\n",
    "\"\"\"\n",
    "\n",
    "import os, json\n",
    "from scrapegraphai.graphs import OmniScraperGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-4o\",\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "    \"headless\": True,\n",
    "    \"max_images\": 5\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the OmniScraperGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "omni_scraper_graph = OmniScraperGraph(\n",
    "    prompt=\"List me all the projects with their titles and image links and descriptions.\",\n",
    "    # also accepts a string with the already downloaded HTML code\n",
    "    source=\"https://perinim.github.io/projects/\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = omni_scraper_graph.run()\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = omni_scraper_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PDFScraperGraph` is distinct from other graph types as it is specifically designed to extract information from PDF files using a natural language model to interpret and answer prompts. The `source` parameter can be a path to a single PDF file or a directory containing multiple PDF files. This flexibility allows it to handle various PDF inputs effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import PyPDF4\n",
    "from scrapegraphai.graphs import PDFScraperGraph\n",
    "\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "FILE_NAME = \"inputs/dante-01-inferno.pdf\"\n",
    "curr_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "file_path = os.path.join(curr_dir, FILE_NAME)\n",
    "\n",
    "# Open the PDF file\n",
    "with open(file_path, \"rb\") as file:\n",
    "    pdf_reader = PyPDF4.PdfFileReader(file)\n",
    "    text = \"\"\n",
    "\n",
    "    # Extract text from each page\n",
    "    for page_num in range(pdf_reader.numPages):\n",
    "        page = pdf_reader.getPage(page_num)\n",
    "        text += page.extractText()\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\"\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "pdf_scraper_graph = PDFScraperGraph(\n",
    "    prompt=\"Summarize the text and find the main topics\",\n",
    "    source=text[:10_000],\n",
    "    config=graph_config,\n",
    ")\n",
    "result = pdf_scraper_graph.run()\n",
    "\n",
    "print(json.dumps(result, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SmartScraperGraph` is distinct from other graph types as it is designed to automate the extraction of information from web pages using a natural language model to interpret and answer prompts. This graph stands out for its ability to handle both web and local directory sources, as indicated by the `source` parameter. The `source` parameter can either be a URL for online content or a local directory path, allowing it to flexibly manage various data inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Basic example of scraping pipeline using SmartScraper from text\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "# ************************************************\n",
    "# Read the text file\n",
    "# ************************************************\n",
    "\n",
    "FILE_NAME = \"inputs/plain_html_example.txt\"\n",
    "curr_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "file_path = os.path.join(curr_dir, FILE_NAME)\n",
    "\n",
    "# It could be also a http request using the request model\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the SmartScraperGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "smart_scraper_graph = SmartScraperGraph(\n",
    "    prompt=\"List me all the projects with their description.\",\n",
    "    source=text,\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = smart_scraper_graph.run()\n",
    "# Print the JSON data in a pretty format\n",
    "pretty_json = json.dumps(result, indent=4)\n",
    "print(pretty_json)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = smart_scraper_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmartScraperGraph vs. OmniScraperGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SmartScraperGraph` and `OmniScraperGraph` are both designed to automate information extraction using natural language models, but they have different capabilities and specific use cases. Here are the key differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SmartScraperGraph**:\n",
    "1. **Primary Focus**: The `SmartScraperGraph` is primarily designed for extracting textual information from web pages or local directories.\n",
    "2. **Nodes and Workflow**:\n",
    "   - Uses `FetchNode` to retrieve documents and URLs.\n",
    "   - Uses `ParseNode` to process and parse the document content.\n",
    "   - Uses `RAGNode` to extract relevant chunks of information.\n",
    "   - Uses `GenerateAnswerNode` to generate final answers from the parsed document and relevant chunks.\n",
    "3. **Image Handling**: While it can fetch image URLs, it does not explicitly process image content or convert images to text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OmniScraperGraph**:\n",
    "1. **Enhanced Capabilities**: The `OmniScraperGraph` has a broader range of capabilities, including handling both textual and visual data. It is designed to integrate and process images as well as text from various sources.\n",
    "2. **Nodes and Workflow**:\n",
    "   - Uses `FetchNode` to retrieve documents, links, and image URLs.\n",
    "   - Uses `ParseNode` to process and parse the document content.\n",
    "   - Uses `ImageToTextNode` to convert image URLs to textual descriptions.\n",
    "   - Uses `RAGNode` to extract relevant chunks of information.\n",
    "   - Uses `GenerateAnswerOmniNode` to generate final answers that integrate both textual content and image descriptions.\n",
    "3. **Image Handling**: Explicitly includes `ImageToTextNode` for processing image URLs and converting them to text, which is then used in generating comprehensive answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script creator graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptCreatorGraph` defines a scraping pipeline that automates the generation of web scraping scripts using a natural language model. It takes a prompt and source (URL or local directory) and processes the content through nodes that fetch and parse the document. The `library` parameter specifies the web scraping library to be used. This parameter ensures the generated script is compatible with the desired web scraping framework. The `GenerateScraperNode` then creates a script tailored to the input prompt, making it a versatile tool for generating web scraping scripts with different libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Basic example of scraping pipeline using ScriptCreatorGraph\n",
    "\"\"\"\n",
    "\n",
    "from scrapegraphai.graphs import ScriptCreatorGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "    \"library\": \"beautifulsoup\"\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the ScriptCreatorGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "script_creator_graph = ScriptCreatorGraph(\n",
    "    prompt=\"List me all the projects with their description.\",\n",
    "    # also accepts a string with the already downloaded HTML code\n",
    "    source=\"https://perinim.github.io/projects\",\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = script_creator_graph.run()\n",
    "print(result)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = script_creator_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script generator schema graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script generator schema graph also creates a script. However, it generates the output according to the pre-defined Pydantic schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Basic example of scraping pipeline using ScriptCreatorGraph\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from scrapegraphai.graphs import ScriptCreatorGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ************************************************\n",
    "# Define the schema for the graph\n",
    "# ************************************************\n",
    "\n",
    "class AttractionSchema(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    image_url: str\n",
    "\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-4o\",\n",
    "    },\n",
    "    \"library\": \"beautifulsoup\",\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "\n",
    "# ************************************************\n",
    "# Create the ScriptCreatorGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "script_creator_graph = ScriptCreatorGraph(\n",
    "    prompt=\"List me the all attractions in Chioggia.\",\n",
    "    source=\"https://en.wikipedia.org/wiki/Chioggia\",\n",
    "    config=graph_config,\n",
    "    schema=AttractionSchema\n",
    ")\n",
    "result = script_creator_graph.run()\n",
    "print(result)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = script_creator_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search graph with schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SearchGraph` module is a powerful tool for dynamically searching the internet and extracting relevant information based on user prompts. It utilizes the language model to perform searches on the internet, making it highly dynamic and capable of retrieving up-to-date information. By leveraging the `SmartScraperGraph`, it handles detailed scraping of each URL, ensuring that the extracted data is relevant and accurate. The module also combines information from multiple sources into a single, refined answer, enhancing the reliability and comprehensiveness of the results. Additionally, it allows for customization through configuration parameters and schemas, making it adaptable to different use cases and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Executing SearchInternet Node ---\n",
      "Search Query: Chioggia famous dishes\n",
      "--- Executing GraphIterator Node with batchsize 16 ---\n",
      "processing graph instances:   0%|          | 0/4 [00:00<?, ?it/s]--- Executing Fetch Node ---\n",
      "--- Executing Fetch Node ---\n",
      "--- Executing Fetch Node ---\n",
      "--- Executing Fetch Node ---\n",
      "--- (Fetching HTML from: https://www.visitchioggia.com/en/taste/chioggian-cuisine/) ---\n",
      "--- (Fetching HTML from: https://www.visitchioggia.com/en/taste/) ---\n",
      "--- (Fetching HTML from: https://www.tasteatlas.com/chioggia) ---\n",
      "--- (Fetching HTML from: https://www.sottomarina.net/gastronomia_uk.htm) ---\n",
      "--- Executing Parse Node ---\n",
      "--- Executing RAG Node ---\n",
      "--- (updated chunks metadata) ---\n",
      "--- (tokens compressed and vector stored) ---\n",
      "--- Executing GenerateAnswer Node ---\n",
      "--- Executing Parse Node ---\n",
      "--- Executing Parse Node ---\n",
      "--- Executing RAG Node ---\n",
      "--- Executing RAG Node ---\n",
      "--- (updated chunks metadata) ---\n",
      "--- (updated chunks metadata) ---\n",
      "--- Executing Parse Node ---\n",
      "--- Executing RAG Node ---\n",
      "--- (updated chunks metadata) ---\n",
      "--- (tokens compressed and vector stored) ---\n",
      "--- Executing GenerateAnswer Node ---\n",
      "\n",
      "\u001b[A--- (tokens compressed and vector stored) ---\n",
      "--- Executing GenerateAnswer Node ---\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A--- (tokens compressed and vector stored) ---\n",
      "--- Executing GenerateAnswer Node ---\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Processing chunks: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n",
      "processing graph instances:  25%|██▌       | 1/4 [00:12<00:37, 12.57s/it]\n",
      "\n",
      "Processing chunks: 100%|██████████| 1/1 [00:04<00:00,  4.14s/it]\n",
      "processing graph instances:  50%|█████     | 2/4 [00:13<00:11,  5.51s/it]\n",
      "\n",
      "\n",
      "Processing chunks: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it]\n",
      "Processing chunks: 100%|██████████| 1/1 [00:14<00:00, 14.60s/it]3.05s/it]\n",
      "processing graph instances: 100%|██████████| 4/4 [00:19<00:00,  4.92s/it]\n",
      "--- Executing MergeAnswers Node ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dishes\": [\n",
      "        {\n",
      "            \"name\": \"Sardines in Sa\\u00f2re\",\n",
      "            \"description\": \"Sardines fried and left in a 'carpione' marinade using Chioggian white onions, A traditional dish of Chioggia made with sardines\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Bigoli in salsa\",\n",
      "            \"description\": \"Delicious bigoli pasta in sauce, A pasta dish with a sauce made from onions, anchovies, and sardines, spaghetti with a sauce of garlic, olive oil, onion, parsley and anchovy fillets.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Stewed cuttlefish\",\n",
      "            \"description\": \"Cuttlefish cooked in ink or stewed, Cuttlefish cooked in a stew with tomatoes and other ingredients\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Moeche frite\",\n",
      "            \"description\": \"Crispy molting crabs, Soft-shell crabs fried in batter, crabs fried in abundant oil\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Seafood risotto\",\n",
      "            \"description\": \"Risotto with seafood ingredients, A risotto dish made with a variety of seafood\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Fritto misto\",\n",
      "            \"description\": \"A famous dish from Osteria Penzo in Chioggia\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Baccal\\u00e0 mantecato\",\n",
      "            \"description\": \"A famous dish from Osteria Penzo in Chioggia\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Linguine all'astice\",\n",
      "            \"description\": \"A famous dish from Trattoria Lanterna Blu in Chioggia\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Spaghetti alle vongole\",\n",
      "            \"description\": \"A famous dish from Trattoria Lanterna Blu in Chioggia\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Boboli de vida\",\n",
      "            \"description\": \"snails with olive oil and parsley\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Granseole\",\n",
      "            \"description\": \"Boiled crab, seasoned with olive oil, lemon and spices.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Sardele salae\",\n",
      "            \"description\": \"raw sardines or anchovies preserved in layers of salt and served with olive oil.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Bibarasse in cassopipa\",\n",
      "            \"description\": \"clams cooked with onions\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Baked scallops\",\n",
      "            \"description\": \"cooked with garlic and parsley in the shell with the addition of brandy.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Broeto\",\n",
      "            \"description\": \"slices of different kind of cooked fish or shellfish in a sauce of oil, onion and vinegar. Served with croutons.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Risoto de sepe\",\n",
      "            \"description\": \"rice with fried or boiled cuttlefish, with the addition of oil and parsley.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Spagheti co le bibarasse\",\n",
      "            \"description\": \"Spaghetti served with clams\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Risoto a la pescatora\",\n",
      "            \"description\": \"rice with pieces of fish cooked in their sauce\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Risoto a la ciosota\",\n",
      "            \"description\": \"rice cooked in a sauce of various specialties of fried and boiled fish with garlic, Parmesan and white wine.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Bisato in tecia\",\n",
      "            \"description\": \"eel in tomato sauce and white wine\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Sepe nere\",\n",
      "            \"description\": \"cuttlefish, boiled in a mixture of onion and garlic, with the addition of white wine, tomatoes and spices.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Sardele in saore\",\n",
      "            \"description\": \"fried sardines or anchovies preserved in a sauce of onions and vinegar\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Risi e vuovi\",\n",
      "            \"description\": \"cuttlefish eggs boiled and seasoned with olive oil, vinegar and spices.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Sepe in umido\",\n",
      "            \"description\": \"cuttlefish in tomato sauce and spices.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Pesse rosto incoverci\\u00e0\",\n",
      "            \"description\": \"many specialty of fish roasted over coals and heated in a covered pan with olive oil, vinegar, wine and garlic.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Radicchio rosso\",\n",
      "            \"description\": \"Can be served with oil and salt; barbecued, or fried\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Papini\",\n",
      "            \"description\": \"thin and hard donuts typical of the Easter\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Sugoli\",\n",
      "            \"description\": \"cream made of black grapes and flour\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Smegiassa\",\n",
      "            \"description\": \"cake made with a mixture of black honey, flour, pumpkin, raisins, pine nuts and sugar\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Bossol\\u00e0\",\n",
      "            \"description\": \"Typical speciality recognized as 'bread of Chioggia', shaped as a ring, fragrant and crisp, easy to maintain.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "        node_name  total_tokens  prompt_tokens  completion_tokens  \\\n",
      "0  SearchInternet           135            130                  5   \n",
      "1   GraphIterator             0              0                  0   \n",
      "2    MergeAnswers          2410           1265               1145   \n",
      "3    TOTAL RESULT          2545           1395               1150   \n",
      "\n",
      "   successful_requests  total_cost_USD  exec_time  \n",
      "0                    1        0.000205   5.189819  \n",
      "1                    0        0.000000  19.685411  \n",
      "2                    1        0.004188  21.039276  \n",
      "3                    2        0.004393  45.914506  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example of Search Graph\n",
    "\"\"\"\n",
    "\n",
    "from scrapegraphai.graphs import SearchGraph\n",
    "from scrapegraphai.utils import convert_to_csv, convert_to_json, prettify_exec_info\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ************************************************\n",
    "# Define the output schema for the graph\n",
    "# ************************************************\n",
    "\n",
    "class Dish(BaseModel):\n",
    "    name: str = Field(description=\"The name of the dish\")\n",
    "    description: str = Field(description=\"The description of the dish\")\n",
    "\n",
    "class Dishes(BaseModel):\n",
    "    dishes: List[Dish]\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "    \"max_results\": 4,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the SearchGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "search_graph = SearchGraph(\n",
    "    prompt=\"List me Chioggia's famous dishes\",\n",
    "    config=graph_config,\n",
    "    schema=Dishes\n",
    ")\n",
    "\n",
    "result = search_graph.run()\n",
    "# Print the JSON data in a pretty format\n",
    "pretty_json = json.dumps(result, indent=4)\n",
    "print(pretty_json)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = search_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n",
    "\n",
    "# Save to json and csv\n",
    "convert_to_csv(result, \"result\")\n",
    "convert_to_json(result, \"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smartscraper graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SmartScraperGraph` module automates the process of extracting information from web pages using a natural language model to interpret and respond to prompts. \n",
    "\n",
    "This module utilizes the language model to perform dynamic internet searches, ensuring the retrieval of up-to-date information. By leveraging the `SmartScraperGraph`, it handles detailed scraping of each URL to ensure that the extracted data is relevant and accurate. The results from multiple sources are combined into a single, refined answer, enhancing the reliability and comprehensiveness of the output. Additionally, the module allows for customization through configuration parameters and schemas, making it adaptable to various use cases and requirements. \n",
    "\n",
    "The `SmartScraperGraph` initializes with a prompt, a source (URL or local directory), a configuration dictionary, and an optional schema for the output format. It uses nodes such as `FetchNode` to retrieve documents, `ParseNode` to process and parse the document, `RAGNode` to extract relevant information, and `GenerateAnswerNode` to generate the final answer formatted according to the optional schema. The workflow of these nodes ensures a seamless data flow from fetching to generating the final answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Executing Fetch Node ---\n",
      "--- (Fetching HTML from: https://perinim.github.io/projects/) ---\n",
      "--- Executing Parse Node ---\n",
      "--- Executing RAG Node ---\n",
      "--- (updated chunks metadata) ---\n",
      "--- (tokens compressed and vector stored) ---\n",
      "--- Executing GenerateAnswer Node ---\n",
      "Processing chunks: 100%|██████████| 1/1 [00:03<00:00,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"projects\": [\n",
      "        {\n",
      "            \"title\": \"Rotary Pendulum RL\",\n",
      "            \"description\": \"Open Source project aimed at controlling a real life rotary pendulum using RL algorithms\"\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"DQN Implementation from scratch\",\n",
      "            \"description\": \"Developed a Deep Q-Network algorithm to train a simple and double pendulum\"\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Multi Agents HAED\",\n",
      "            \"description\": \"University project which focuses on simulating a multi-agent system to perform environment mapping. Agents, equipped with sensors, explore and record their surroundings, considering uncertainties in their readings.\"\n",
      "        },\n",
      "        {\n",
      "            \"title\": \"Wireless ESC for Modular Drones\",\n",
      "            \"description\": \"Modular drone architecture proposal and proof of concept. The project received maximum grade.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "        node_name  total_tokens  prompt_tokens  completion_tokens  \\\n",
      "0           Fetch             0              0                  0   \n",
      "1           Parse             0              0                  0   \n",
      "2             RAG             0              0                  0   \n",
      "3  GenerateAnswer           691            518                173   \n",
      "4    TOTAL RESULT           691            518                173   \n",
      "\n",
      "   successful_requests  total_cost_USD  exec_time  \n",
      "0                    0        0.000000  14.398365  \n",
      "1                    0        0.000000   0.012759  \n",
      "2                    0        0.000000   1.252254  \n",
      "3                    1        0.001123   3.178386  \n",
      "4                    1        0.001123  18.841764  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Basic example of scraping pipeline using SmartScraper\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "    \"headless\": False,\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the SmartScraperGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "smart_scraper_graph = SmartScraperGraph(\n",
    "    prompt=\"List me all the projects with their description\",\n",
    "    # also accepts a string with the already downloaded HTML code\n",
    "    source=\"https://perinim.github.io/projects/\",\n",
    "    config=graph_config,\n",
    ")\n",
    "\n",
    "result = smart_scraper_graph.run()\n",
    "print(json.dumps(result, indent=4))\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = smart_scraper_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate audio from the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpeechGraph` module automates the process of web scraping, generating answers to user prompts, and converting those answers into audio files. This pipeline integrates several nodes to create a comprehensive workflow, allowing it to fetch, parse, interpret, and transform web content into spoken word.\n",
    "\n",
    "Initially, the `SpeechGraph` is set up with a prompt, a source (URL or local directory), a configuration dictionary, and an optional schema for the output format. The workflow involves multiple nodes: the `FetchNode` retrieves the document and associated links and images from the source, the `ParseNode` processes the fetched document into a parsed version, and the `RAGNode` extracts relevant chunks of information from the parsed document. The `GenerateAnswerNode` then creates a final answer based on the user prompt and relevant information. Lastly, the `TextToSpeechNode` converts the generated answer into an audio file using a text-to-speech model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Basic example of scraping pipeline using SpeechSummaryGraph\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from scrapegraphai.graphs import SpeechGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "# ************************************************\n",
    "# Define audio output path\n",
    "# ************************************************\n",
    "\n",
    "FILE_NAME = \"website_summary.mp3\"\n",
    "curr_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "output_path = os.path.join(curr_dir, FILE_NAME)\n",
    "\n",
    "# ************************************************\n",
    "# Define the configuration for the graph\n",
    "# ************************************************\n",
    "\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": 0.7,\n",
    "    },\n",
    "    \"tts_model\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"tts-1\",\n",
    "        \"voice\": \"alloy\"\n",
    "    },\n",
    "    \"output_path\": output_path,\n",
    "}\n",
    "\n",
    "# ************************************************\n",
    "# Create the SpeechGraph instance and run it\n",
    "# ************************************************\n",
    "\n",
    "speech_graph = SpeechGraph(\n",
    "    prompt=\"Make a detailed audio summary of the projects.\",\n",
    "    source=\"https://perinim.github.io/projects/\",\n",
    "    config=graph_config,\n",
    ")\n",
    "\n",
    "result = speech_graph.run()\n",
    "print(result)\n",
    "\n",
    "# ************************************************\n",
    "# Get graph execution info\n",
    "# ************************************************\n",
    "\n",
    "graph_exec_info = speech_graph.get_execution_info()\n",
    "print(prettify_exec_info(graph_exec_info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a graph from a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"input\": \"Create a graph that extracts dynamic content from a given web site\",\n",
      "    \"text\": [\n",
      "        {\n",
      "            \"nodes\": [\n",
      "                {\n",
      "                    \"node_name\": \"SearchInternetNode\",\n",
      "                    \"node_type\": \"node\"\n",
      "                },\n",
      "                {\n",
      "                    \"node_name\": \"FetchNode\",\n",
      "                    \"node_type\": \"node\"\n",
      "                },\n",
      "                {\n",
      "                    \"node_name\": \"RAGNode\",\n",
      "                    \"node_type\": \"node\"\n",
      "                },\n",
      "                {\n",
      "                    \"node_name\": \"ParseNode\",\n",
      "                    \"node_type\": \"node\"\n",
      "                }\n",
      "            ],\n",
      "            \"edges\": [\n",
      "                {\n",
      "                    \"from\": \"SearchInternetNode\",\n",
      "                    \"to\": [\n",
      "                        \"FetchNode\"\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"from\": \"FetchNode\",\n",
      "                    \"to\": [\n",
      "                        \"RAGNode\"\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"from\": \"RAGNode\",\n",
      "                    \"to\": [\n",
      "                        \"ParseNode\"\n",
      "                    ]\n",
      "                }\n",
      "            ],\n",
      "            \"entry_point\": \"SearchInternetNode\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from scrapegraphai.builders import GraphBuilder\n",
    "\n",
    "# Define the graph configuration\n",
    "config = {\n",
    "  \"llm\": {\n",
    "      \"api_key\": openai_key,\n",
    "      \"model\": \"gpt-3.5-turbo\",\n",
    "      \"temperature\": 0.7,\n",
    "  }\n",
    "}\n",
    "\n",
    "# Create and run the graph\n",
    "graph_builder = GraphBuilder(\n",
    "  user_prompt=\"Create a graph that extracts dynamic content from a given web site\",\n",
    "  config=config,\n",
    "  )\n",
    "result = graph_builder.build_graph()\n",
    "# Print the JSON data in a pretty format\n",
    "pretty_json = json.dumps(result, indent=4)\n",
    "print(pretty_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
